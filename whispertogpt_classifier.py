# -*- coding: utf-8 -*-
"""WhisperToGPT Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EzPrAKPEvfdnz5JQP-cp3VUTUgSltOK6

# Whisper w/ Speech Classifier
Imports/Installations
"""

# Don't run this cell
!rm -rf Training\ Data

# for audio processing/whisper

!pip install -q git+https://github.com/openai/whisper.git > /dev/null
!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null
!pip install pydub

import whisper
import datetime
import subprocess
import torch
import pyannote.audio
import os
from zipfile import ZipFile
from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding
embedding_model = PretrainedSpeakerEmbedding(
    "speechbrain/spkrec-ecapa-voxceleb",
    device=torch.device("cuda"))

from pyannote.audio import Audio
from pyannote.core import Segment

import wave
import contextlib

from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Bunch of different classifiers to try out
from sklearn.svm import SVC # Remove if using different classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

from pydub import AudioSegment
from pydub.silence import detect_silence
from pydub.silence import split_on_silence
from pydub import AudioSegment, effects
from scipy.io.wavfile import read, write

"""Load Model"""

model_size, model_name = "large", "large"


# load model
model = whisper.load_model(model_size) # this line may take a sec

path_to_zip_file = '/content/Training Data.zip'
destination_folder = '/content/Training Data/'

# Unzip the folder
with ZipFile(path_to_zip_file, 'r') as zip_ref:
    zip_ref.extractall(destination_folder)
!rm -rf Training\ Data/__MACOSX

# Delete the original zip file
os.remove(path_to_zip_file)

"""Create Embeddings using Data"""

def create_embeddings(path, original_path):
  # transcribe
  # give in individual segments with timestamps.
  result = model.transcribe(path)
  segments = result["segments"]

  with contextlib.closing(wave.open(path, 'r')) as f:
    frames = f.getnframes()
    rate = f.getframerate()
    duration = frames / float(rate)

  audio = Audio()

  # create embedding using embedding model for each segment
  def segment_embedding(segment):
    start = segment["start"]
    # Whisper overshoots the end timestamp in the last segment
    end = min(duration, segment["end"])
    clip = Segment(start, end)
    waveform, sample_rate = audio.crop(path, clip)
    return embedding_model(waveform[None])

  embeddings = np.zeros(shape=(len(segments), 192))
  # Store the inverse mapping embedding -> segment
  # This is for testing verification purposes
  embeddings_to_segment_map = {}  # Dictionary to store the mapping
  for i, segment in enumerate(segments):
    embeddings[i] = segment_embedding(segment)
    embeddings_to_segment_map[tuple(embeddings[i])] = (segment, original_path)


  embeddings = np.nan_to_num(embeddings)

  # Show transcript



  return embeddings, segments, embeddings_to_segment_map

def time(secs):
    return datetime.timedelta(seconds=round(secs))
    # used for transcript later - see clustering file for where it's associated

def get_audio_paths(root_dir):
    audio_paths_by_label = {}
    for label in os.listdir(root_dir):
        label_dir = os.path.join(root_dir, label)
        if os.path.isdir(label_dir):
            audio_paths = []
            for subdir, _, files in os.walk(label_dir):
                for file in files:
                    # Add audio file paths with appropriate extensions (you can modify this for other audio file formats)
                    if file.endswith('.wav') or file.endswith('.mp3'):
                        audio_path = os.path.join(subdir, file)
                        audio_paths.append(audio_path)
            audio_paths_by_label[label] = audio_paths
    return audio_paths_by_label



# This is where we actually load the audio and call above functions.
root_directory = '/content/Training Data/Training Data'
audio_paths_by_label = get_audio_paths(root_directory)

X = []
Y = []

# Now, 'audio_paths_by_label' is a dictionary where each key is a label, and the corresponding value is a list of audio file paths for that label
embeddings_to_segment_map = {}
for label, audio_paths in audio_paths_by_label.items():
    print(f"Label: {label}")
    print("Audio Paths:")
    print(audio_paths[0].split('/')[-1]) # same as original_path variable created later on
    print()

    for filepath in audio_paths:
      subprocess.call(['ffmpeg', '-i', filepath, 'audio.wav', '-y'])
      path = 'audio.wav'
      original_path = audio_paths[0].split('/')[-1]
      emb, segs, emb_map = create_embeddings(path, original_path) # original path used for mapping embeddings to segments
      embeddings_to_segment_map.update(emb_map)
      # print(emb)
      X += list(emb)
      Y += [label] * len(emb)
      # print(f"len(X): {len(X)}")
      # print(f"len(Y): {len(Y)}")

      # Construct actual data set


    for i in range(len(segs)):
      segs[i]["speaker"] = 'SPEAKER ' + str(label)

    '''
    # Print transcript - not needed for training.
    transcript_filename = label + "_transcript.txt"
    f = open(transcript_filename, "w")

    for (i, segment) in enumerate(segs):
      if i == 0 or segs[i - 1]["speaker"] != segment["speaker"]:
        f.write("\n" + segment["speaker"] + ' ' + str(time(segment["start"])) + '\n')
      f.write(segment["text"][1:] + ' ')
    f.close()
    '''

"""Construct Classifier, Classify test data"""

# To verify shape: X has 192 dim embedding associated w/ Y label
# print([len(x) for x in X])
# print(Y)
# print(len(X))
# print(len(Y))

# Now that dataset constructed, do train-test split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42) # to create final model for classification change test size to 0.01

# save the origina x_test so we can use embedding map
X_train_numpy = np.array(X_train)
X_test_numpy = np.array(X_test)
y_train_numpy = np.array(y_train)
y_test_numpy = np.array(y_test)


# Scale data (may not be needed)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_numpy)
X_test_scaled = scaler.transform(X_test_numpy)

# Use classifier (try out variety)

classifier = SVC(kernel='linear', C=1.0) # can try rbf kernel too
# classifier = RandomForestClassifier(n_estimators=100, random_state=42)
# classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
# classifier = KNeighborsClassifier(n_neighbors=5)
# classifier = DecisionTreeClassifier(max_depth=3, random_state=42)
# classifier = LogisticRegression(max_iter=1000, random_state=42)
# classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)

# Experiment with more classifiers if accuracy not good.

classifier.fit(X_train_scaled, y_train_numpy)

# Predict
y_pred = classifier.predict(X_test_scaled)

# Check performance
accuracy = accuracy_score(y_test_numpy, y_pred)
print("Accuracy:", accuracy)

print("Classification Report:")
print(classification_report(y_test_numpy, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test_numpy, y_pred))



for i in range(len(y_test)):
  print(f"predicted value: {y_pred[i]}, actual value: {y_test[i]}")
  segment, recording = embeddings_to_segment_map[tuple(X_test[i])]
  recording = recording[23:27] # only care about the date. Parse it
  # if you want, include year by using indices 19-27. Insert '-' differently below
  print(f"segment: {segment['text']}, start: {segment['start']};  end: {segment['end']};  recording: {recording[:2] + '-' + recording[2:]}\n")
  print()
# print(X_test[0])
# print(embeddings_to_segment_map)

"""Classifier created, save model. If the accuracy isn't high enough, refine model before using it."""

# save model
file_path = 'saved_classifier.joblib'
joblib.dump(classifier, file_path)
print("Classifier saved to:", file_path)

# Clean up runtime (optional)

!ls Training\ Data/
!rm audio.wav

"""Load Model, Construct Meeting Transcript"""

# NO NEED TO RUN NEXT 6 CELLS IF ALREADY IMPORTED AT THE TOP
# THESE CELLS ARE IF STARTING HERE AFTER LOADING SAVED MODEL

# For GPT

!pip install openai --quiet
!pip install flask --quiet
!pip show openai
!pip install joblib
!pip install pydub
!pip install -q git+https://github.com/openai/whisper.git > /dev/null
!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null

import whisper
import datetime
import subprocess
import torch
import pyannote.audio
import os
from zipfile import ZipFile
from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding
embedding_model = PretrainedSpeakerEmbedding(
    "speechbrain/spkrec-ecapa-voxceleb",
    device=torch.device("cuda"))

from pyannote.audio import Audio
from pyannote.core import Segment

import wave
import contextlib

from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Bunch of different classifiers to try out
from sklearn.svm import SVC # Remove if using different classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

from pydub import AudioSegment
from pydub.silence import detect_silence
from pydub.silence import split_on_silence
from pydub import AudioSegment, effects
from scipy.io.wavfile import read, write

def time(secs):
    return datetime.timedelta(seconds=round(secs))
    # used for transcript later - see clustering file for where it's associated

model_size, model_name = "large", "large"


# load model
model = whisper.load_model(model_size) # this line may take a sec

file_path = 'saved_classifier.joblib'
# Saving and loading part is optional since we are in same file
diarization_classifier = joblib.load(file_path)

def create_embeddings(path, original_path):
  # transcribe
  # give in individual segments with timestamps.
  result = model.transcribe(path)
  segments = result["segments"]

  with contextlib.closing(wave.open(path, 'r')) as f:
    frames = f.getnframes()
    rate = f.getframerate()
    duration = frames / float(rate)

  audio = Audio()

  # create embedding using embedding model for each segment
  def segment_embedding(segment):
    start = segment["start"]
    # Whisper overshoots the end timestamp in the last segment
    end = min(duration, segment["end"])
    clip = Segment(start, end)
    waveform, sample_rate = audio.crop(path, clip)
    return embedding_model(waveform[None])

  embeddings = np.zeros(shape=(len(segments), 192))
  # Store the inverse mapping embedding -> segment
  # This is for testing verification purposes
  embeddings_to_segment_map = {}  # Dictionary to store the mapping
  for i, segment in enumerate(segments):
    embeddings[i] = segment_embedding(segment)
    embeddings_to_segment_map[tuple(embeddings[i])] = (segment, original_path)


  embeddings = np.nan_to_num(embeddings)

  # Show transcript



  return embeddings, segments, embeddings_to_segment_map

# If doing this stuff in separate file, need to copy over the respective functions called.
# If too much CPU or GPU memory is used in training, make new file.

file_path = "Recording.mp3"

subprocess.call(['ffmpeg', '-i', file_path, 'audio.wav', '-y'])
path = 'audio.wav'

#Take out silence
file_path = "/content/audio.wav"
file_name = file_path.split('/')[-1]
audio_format = "wav"

# Reading and splitting the audio file into chunks
sound = AudioSegment.from_file(file_path, format = audio_format)
audio_chunks = split_on_silence(sound
                            ,min_silence_len = 100
                            ,silence_thresh = -45
                            ,keep_silence = 50
                        )

# Putting the file back together
combined = AudioSegment.empty()
for chunk in audio_chunks:
    combined += chunk
combined.export(f'./{file_name}', format = audio_format)


emb, segs, _ = create_embeddings(path, "") # ignore original_path param, only for training.

# Previous cell takes time so make new cell

X = []

X += list(emb)
X = np.array(X)
scaler = StandardScaler()
X = scaler.fit_transform(X)


pred_labels = diarization_classifier.predict(X)
for i in range(len(segs)):
    segs[i]["speaker"] = 'SPEAKER ' + str(pred_labels[i])

# Print transcript
transcript_filename = "transcript.txt"
f = open(transcript_filename, "w")


for (i, segment) in enumerate(segs):
  if i == 0 or segs[i - 1]["speaker"] != segment["speaker"]:
    f.write("\n" + segment["speaker"] + ' ' + str(time(segment["start"])) + '\n')
  f.write(segment["text"][1:] + ' ')
f.close()